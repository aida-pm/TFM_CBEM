{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 11 12:59:49 2020\n",
    "\n",
    "@author: javi485\n",
    "\"\"\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_training = \"/homedtic/xmorales/Seg/nnUNet/nnUNet_raw_data_base/nnUNet_raw_data/Task001_LALAA/Tranining\"\n",
    "    folder_test = \"/homedtic/xmorales/Seg/nnUNet/nnUNet_raw_data_base/nnUNet_raw_data/Task001_LALAA/Testing\"\n",
    "    output_folder = \"/homedtic/xmorales/Seg/nnUNet/nnUNet_raw_data_base/nnUNet_raw_data/Task001_LALAA\"\n",
    "    \n",
    "    maybe_mkdir_p(join(output_folder, \"imagesTr\"))\n",
    "    maybe_mkdir_p(join(output_folder, \"imagesTs\"))\n",
    "    maybe_mkdir_p(join(output_folder, \"labelsTr\"))\n",
    "    \n",
    "     # train\n",
    "    all_train_files = []\n",
    "    data_files_train = [i for i in subfiles(folder_training, suffix=\".nii.gz\")if i.find(\"_m.nii.gz\")==-1]\n",
    "    corresponding_seg_files = [i[:-7] + \"_m.nii.gz\" for i in data_files_train]\n",
    "    for d, s in zip(data_files_train, corresponding_seg_files):\n",
    "        patient_identifier = d.split(\"/\")[-1][:-7]\n",
    "        all_train_files.append(patient_identifier + \"_0000.nii.gz\")\n",
    "        shutil.copy(d, join(output_folder, \"imagesTr\", patient_identifier + \"_0000.nii.gz\").replace(\"\\\\\",\"/\"))\n",
    "        shutil.copy(s, join(output_folder, \"labelsTr\", patient_identifier + \".nii.gz\").replace(\"\\\\\",\"/\"))\n",
    "            \n",
    "    # test\n",
    "    all_test_files = []\n",
    "    data_files_test = [i for i in subfiles(folder_test, suffix=\".nii.gz\") if i.find(\"_m.nii.gz\")==-1 ]\n",
    "    corresponding_seg_files_ts = [i[:-7] + \".nii.gz\" for i in data_files_test]\n",
    "    for d in data_files_test:\n",
    "        patient_identifier = d.split(\"/\")[-1][:-7]\n",
    "        all_test_files.append(patient_identifier + \"_0000.nii.gz\")\n",
    "        shutil.copy(d, join(output_folder, \"imagesTs\", patient_identifier + \"_0000.nii.gz\").replace(\"\\\\\",\"/\"))\n",
    "        \n",
    "    json_dict = OrderedDict()\n",
    "    json_dict['name'] = \"NZ\"\n",
    "    json_dict['description'] = \"Burdeos dataset left atrium segmentation\"\n",
    "    json_dict['tensorImageSize'] = \"3D\"\n",
    "    json_dict['reference'] = \"see MICCAI challenge\"\n",
    "    json_dict['licence'] = \"see MICCAI challenge\"\n",
    "    json_dict['release'] = \"0.0\"\n",
    "    json_dict['modality'] = {\n",
    "        \"0\": \"CT\"\n",
    "    }\n",
    "    json_dict['labels'] = {\n",
    "        \"0\": \"background\",\n",
    "        \"1\": \"LALAA\"\n",
    "    }\n",
    "    json_dict['numTraining'] = len(all_train_files)\n",
    "    json_dict['numTest'] = len(all_test_files)\n",
    "    json_dict['training'] = [{'image': \"./imagesTr/%s.nii.gz\" % i.split(\"/\")[-1][:-12], \"label\": \"./labelsTr/%s.nii.gz\" % i.split(\"/\")[-1][:-12]} for i in\n",
    "                             all_train_files]\n",
    "    json_dict['test'] = [\"./imagesTs/%s.nii.gz\" % i.split(\"/\")[-1][:-12] for i in all_test_files]\n",
    "\n",
    "    save_json(json_dict, os.path.join(output_folder, \"dataset.json\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
